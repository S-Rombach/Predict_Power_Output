{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Plot a comparison of all evaluated models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Load All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.config import SOLAR_PROD_DAILY_MODELS_DIR as MODELS_DIR, MODEL_ALIASES\n",
    "\n",
    "error_metrics = [\"MAE\", \"MSE\", \"RMSE\", \"MAPE\", \"MedAE\"]\n",
    "score_metrics = [\"R2\", \"ExplainedVar\"]\n",
    "\n",
    "target_keys = (\n",
    "    [\n",
    "        \"timestamp\",\n",
    "        \"model_purpose\",\n",
    "        \"special_features\",\n",
    "        \"model_class\",\n",
    "    ]\n",
    "    + error_metrics\n",
    "    + score_metrics\n",
    ")\n",
    "\n",
    "subdirs = [\n",
    "    d for d in os.listdir(MODELS_DIR) if os.path.isdir(d) and not d.startswith(\".\")\n",
    "]\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for d in subdirs:\n",
    "    result_path = os.path.join(MODELS_DIR, d, f\"{d}.results.json\")\n",
    "    row = {}\n",
    "    if os.path.exists(result_path):\n",
    "        with open(result_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for key in target_keys:\n",
    "            val = data\n",
    "            for part in key.split(\"__\"):\n",
    "                if isinstance(val, dict) and part in val:\n",
    "                    val = val[part]\n",
    "                else:\n",
    "                    val = None\n",
    "                    break\n",
    "            row[key] = val\n",
    "    else:\n",
    "        continue\n",
    "    row[\"dir\"] = d\n",
    "    rows.append(row)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "\n",
    "\n",
    "df = df.sort_values(by=\"R2\", ascending=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Print A Markdown Version Of The DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df as markdown table\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate(df.reset_index(), headers=\"keys\", tablefmt=\"github\", showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "colors = list(cmap.colors)\n",
    "\n",
    "linestyles = [\"-\", \"--\", \"-.\", \":\"]\n",
    "markers = [\"o\", \"s\", \"D\", \"^\", \"v\", \"<\", \">\", \"x\", \"+\", \"*\", \"p\", \"h\"]\n",
    "\n",
    "def style_generator(markers=markers, linestyles=linestyles, colors=colors):\n",
    "    for marker in markers:\n",
    "        for linestyle in linestyles:\n",
    "            for color in colors:\n",
    "                yield {\"color\": color, \"linestyle\": linestyle, \"marker\": marker}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Function For Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback for row labels\n",
    "def get_row_label(row):\n",
    "    name = row.name.strftime(\"%m-%d %H:%M:%S\") + \" \"\n",
    "\n",
    "    if pd.notna(row.get(\"model_class\")) and row[\"model_class\"] != \"\":\n",
    "        if row[\"model_class\"] in MODEL_ALIASES:\n",
    "            name += MODEL_ALIASES[str(row[\"model_class\"])]\n",
    "        else:\n",
    "            name += str(row[\"model_class\"])\n",
    "        name += \" \"\n",
    "\n",
    "    if pd.notna(row.get(\"special_features\")) and row[\"special_features\"] != \"\":\n",
    "        name += str(row[\"special_features\"])\n",
    "    else:\n",
    "        name += str(row.get(\"model_purpose\", \"\"))\n",
    "\n",
    "\n",
    "    name = f\"{name}\"\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_model(\n",
    "    row, label, ax, metrics, annotate=False, **kwargs\n",
    "):\n",
    "    (line,) = ax.plot(\n",
    "        metrics,\n",
    "        row[metrics],\n",
    "        label=label,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    if annotate:\n",
    "        for xx, col_name in enumerate(metrics):\n",
    "            value = float(row[col_name])\n",
    "            ax.annotate(\n",
    "                f\"{value:.2f}\",\n",
    "                xy=(xx, value),\n",
    "                xytext=(5, 5),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"left\",\n",
    "                color=line.get_color(),\n",
    "                fontsize=9,\n",
    "            )\n",
    "\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Plot Each Models Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show = df.copy()\n",
    "df_baseline = df_show[df_show[\"model_purpose\"] == \"baseline\"]\n",
    "\n",
    "# limit error metrics to avoid too large values in the plot\n",
    "upper_y_limit_show = np.floor(\n",
    "    float(df_baseline[error_metrics].max().multiply(10).max())\n",
    ")\n",
    "\n",
    "df_show[error_metrics] = df_show[error_metrics].clip(upper=upper_y_limit_show)\n",
    "df_show = df_show[df_show[\"model_purpose\"] != \"baseline\"]\n",
    "\n",
    "styles = style_generator()\n",
    "\n",
    "# Create figure and axis\n",
    "fig, axes = plt.subplots(\n",
    "    figsize=(16, 5),\n",
    "    ncols=2,\n",
    "    gridspec_kw={\"width_ratios\": [len(error_metrics), len(score_metrics)]},\n",
    ")\n",
    "lines = {ax: [] for ax in axes}\n",
    "alpha = 0.9\n",
    "\n",
    "# Plot baseline(s)\n",
    "baseline_style = style_generator(colors=[\"black\"])\n",
    "if not df_baseline.empty:\n",
    "    for idx, row in df_baseline.iterrows():\n",
    "        label = f\"baseline: {get_row_label(row)}\"\n",
    "        style = next(baseline_style)\n",
    "\n",
    "        for ax, metrics in zip(axes, [error_metrics, score_metrics]):\n",
    "            line = plot_single_model(\n",
    "                row,\n",
    "                label if ax == axes[0] else None,  # only one label in legend\n",
    "                ax,\n",
    "                metrics,\n",
    "                False,\n",
    "                **(style | {\"alpha\": alpha}),\n",
    "            )\n",
    "            lines[ax].append((idx, line, row))\n",
    "\n",
    "# Plot each row\n",
    "for idx, row in df_show.iterrows():\n",
    "    label = get_row_label(row)\n",
    "    style = next(styles)\n",
    "    for ax, metrics in zip(axes, [error_metrics, score_metrics]):\n",
    "        line = plot_single_model(\n",
    "            row,\n",
    "            label if ax == axes[0] else None,  # only one label in legend\n",
    "            ax,\n",
    "            metrics,\n",
    "            False,\n",
    "            **(style | {\"alpha\": alpha}),\n",
    "        )\n",
    "\n",
    "        lines[ax].append((idx, line, row))\n",
    "\n",
    "# Mark best value per column in the color of the best line\n",
    "for ax, metrics, y_lim_max in zip(axes, [error_metrics, score_metrics], [None, 1]):\n",
    "    for col in metrics:\n",
    "        best_idx = (\n",
    "            df_show[col].idxmax() if col in score_metrics else df_show[col].idxmin()\n",
    "        )\n",
    "        row = df_show.loc[best_idx]\n",
    "        y = row[col] + (0.05 if col in score_metrics else -0.15)\n",
    "        x = list(metrics).index(col)\n",
    "\n",
    "        # Get the color of the corresponding line\n",
    "        line_color = next(\n",
    "            (l.get_color() for i, l, r in lines[ax] if i == best_idx), \"black\"\n",
    "        )\n",
    "        ax.plot(\n",
    "            x,\n",
    "            y,\n",
    "            marker=\"v\" if col in score_metrics else \"^\",\n",
    "            color=line_color,\n",
    "            markersize=8,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "\n",
    "# Axis and legend settings\n",
    "for ax, metrics, y_lim_max in zip(\n",
    "    axes,\n",
    "    [error_metrics, score_metrics],\n",
    "    [min(np.floor(upper_y_limit_show/5), df_show.loc[:, error_metrics].max().max()), 1],\n",
    "):\n",
    "\n",
    "    ax.set_xticks(range(len(metrics)))\n",
    "    ax.set_xticklabels(metrics, rotation=0, ha=\"center\")\n",
    "\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.set_ylim(ymin=0, ymax=y_lim_max)\n",
    "    ax.set_title(\"Error Metrics\" if metrics == error_metrics else \"Score Metrics\")\n",
    "    ax.set_ylabel(\"MSE: $(W_ph)^2$, else $W_ph$\" if metrics == error_metrics else \"Score\")\n",
    "    # ax.legend()\n",
    "\n",
    "fig.legend(bbox_to_anchor=(0.5, -0.01), loc=\"upper center\", ncol=min(1, len(df_show)))\n",
    "fig.suptitle(\"Comparison of models by selected metrics\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Plot A Chosen Models Metrics Vs The Aggregation Of All Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model_timestamp = None  # e.g. \"2023-10-01 12:00:00\"\n",
    "\"\"\"The timestamp of the target model to highlight. If None or empty, the latest model is used.\"\"\"\n",
    "\n",
    "df_show = df.copy()\n",
    "df_baseline = df_show[df_show[\"model_purpose\"] == \"baseline\"]\n",
    "# limit error metrics to avoid too large values in the plot\n",
    "upper_y_limit_show = np.floor(\n",
    "    float(df_baseline[error_metrics].max().multiply(10).max())\n",
    ")\n",
    "\n",
    "df_agg = (\n",
    "    df[error_metrics + score_metrics]\n",
    "    .clip(upper=upper_y_limit_show)  # to avoid too large values in error metrics\n",
    "    .agg([\"min\", \"max\", \"median\"])\n",
    ")\n",
    "\n",
    "df_show[error_metrics] = df_show[error_metrics].clip(upper=upper_y_limit_show)\n",
    "df_show = df_show[df_show[\"model_purpose\"] != \"baseline\"]\n",
    "\n",
    "\n",
    "styles = style_generator()\n",
    "\n",
    "# Create figure and axis\n",
    "fig, axes = plt.subplots(\n",
    "    figsize=(16, 5),\n",
    "    ncols=2,\n",
    "    gridspec_kw={\"width_ratios\": [len(error_metrics), len(score_metrics)]},\n",
    ")\n",
    "lines = {ax: [] for ax in axes}\n",
    "alpha = 0.9\n",
    "\n",
    "# Plot range and median of metrics\n",
    "for ax, metrics in zip(axes, [error_metrics, score_metrics]):\n",
    "    mean_vals = df_agg[metrics].loc[\"median\"]\n",
    "    (line,) = ax.plot(\n",
    "        metrics,\n",
    "        mean_vals,\n",
    "        label=\"median\" if ax == axes[0] else None,  # only one label in legend\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        color=\"gray\",\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        metrics,\n",
    "        df_agg[metrics].loc[\"min\"],\n",
    "        df_agg[metrics].loc[\"max\"],\n",
    "        alpha=0.3,\n",
    "        label=\"min/max\" if ax == axes[0] else None,  # only one label in legend\n",
    "        color=\"lightgray\",\n",
    "    )\n",
    "\n",
    "# Plot baseline(s)\n",
    "baseline_style = style_generator(colors=[\"black\"])\n",
    "if not df_baseline.empty:\n",
    "    for idx, row in df_baseline.iterrows():\n",
    "        label = f\"baseline: {get_row_label(row)}\"\n",
    "        style = next(baseline_style)\n",
    "\n",
    "        for ax, metrics in zip(axes, [error_metrics, score_metrics]):\n",
    "            line = plot_single_model(\n",
    "                row,\n",
    "                label if ax == axes[0] else None,  # only one label in legend\n",
    "                ax,\n",
    "                metrics,\n",
    "                False,\n",
    "                **(style | {\"alpha\": alpha}),\n",
    "            )\n",
    "            lines[ax].append((idx, line, row))\n",
    "\n",
    "if target_model_timestamp is None or target_model_timestamp == \"\":\n",
    "    target_model_timestamp = df_show.index.max()\n",
    "elif target_model_timestamp not in df_show.index:\n",
    "    raise ValueError(f\"timestamp '{target_model_timestamp}' not found in df.index\")\n",
    "\n",
    "\n",
    "# Plot single model\n",
    "for idx, row in df_show.loc[df_show.index == target_model_timestamp, :].iterrows():\n",
    "    label = get_row_label(row)\n",
    "    style = next(styles)\n",
    "    for ax, metrics in zip(axes, [error_metrics, score_metrics]):\n",
    "        line = plot_single_model(\n",
    "            row,\n",
    "            label if ax == axes[0] else None,  # only one label in legend\n",
    "            ax,\n",
    "            metrics,\n",
    "            True,\n",
    "            **(style | {\"alpha\": alpha}),\n",
    "        )\n",
    "\n",
    "# Axis and legend settings\n",
    "for ax, metrics, y_lim_max in zip(\n",
    "    axes,\n",
    "    [error_metrics, score_metrics],\n",
    "    [min(np.floor(upper_y_limit_show/5), df_show.loc[:, error_metrics].max().max()), 1],\n",
    "):\n",
    "\n",
    "    ax.set_xticks(range(len(metrics)))\n",
    "    ax.set_xticklabels(metrics, rotation=0, ha=\"center\")\n",
    "\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.set_ylim(ymin=0, ymax=y_lim_max)\n",
    "    ax.set_title(\"Error Metrics\" if metrics == error_metrics else \"Score Metrics\")\n",
    "    ax.set_ylabel(\"MSE: $(W_ph)^2$, else $W_ph$\" if metrics == error_metrics else \"Score\")\n",
    "\n",
    "fig.legend(bbox_to_anchor=(0.5, -0.01), loc=\"upper center\", ncol=min(1, len(df_show)))\n",
    "fig.suptitle(\"Comparison of models by selected metrics\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict-power-output",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
